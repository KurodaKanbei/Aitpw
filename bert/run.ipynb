{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"47488e80"},"outputs":[],"source":["#!pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n","!pip install transformers\n","!pip install datasets\n","!pip install sentencepiece"],"id":"47488e80"},{"cell_type":"code","execution_count":null,"metadata":{"id":"00844541"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","from ast import arg\n","from transformers import AutoModelForSequenceClassification\n","from datasets.dataset_dict import DatasetDict\n","from datasets import load_dataset\n","from datasets import load_metric\n","from transformers import TrainingArguments, Trainer\n","import torch\n","import numpy as np\n","from torch.utils.data import DataLoader\n","import os\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch\n","import os\n","#import torch_xla\n","#import torch_xla.core.xla_model as xm\n","#import torch_xla.distributed.xla_multiprocessing as xmp"],"id":"00844541"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce543c80"},"outputs":[],"source":["if not os.path.exists('drive/MyDrive/models'):\n","  os.makedirs('drive/MyDrive/models')"],"id":"ce543c80"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1e3d4f31"},"outputs":[],"source":["print(\"torch using \"+str(torch.get_num_threads())+\" threads\")\n","\n","#SERIAL_EXEC = xmp.MpSerialExecutor()\n","device=\"cuda\"\n","data_dir=\"drive/MyDrive/data/\"\n","batch_size = 16\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n"],"id":"1e3d4f31"},{"cell_type":"markdown","metadata":{"id":"FYQJrlXRYvGx"},"source":[""],"id":"FYQJrlXRYvGx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCY8X_y-6yum"},"outputs":[],"source":["train_dataset=load_dataset('json', data_files=data_dir+\"train.json\")[\"train\"]\n","eval_dataset=load_dataset('json', data_files=data_dir+\"test.json\")[\"train\"]"],"id":"rCY8X_y-6yum"},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjxdAw048wBn"},"outputs":[],"source":["train_loader = DataLoader(\n","                train_dataset,\n","                batch_size=batch_size,\n","                #collate_fn=data_collator,\n","                num_workers=4,\n","                drop_last=True\n","            )\n","\n","eval_loader = DataLoader(\n","                eval_dataset,\n","                batch_size=batch_size,\n","                #collate_fn=data_collator,\n","                num_workers=4,\n","                drop_last=True\n","            )\n","\n"],"id":"XjxdAw048wBn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2FE0xhn58gG"},"outputs":[],"source":["epochs = 3\n","loss = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","model_dir=\"drive/MyDrive/models/\"\n","for i in range(epochs):\n","  # training step\n","  model.train()\n","  for idx, tokenized_examples in enumerate(tqdm(train_loader)):\n","    optimizer.zero_grad()\n","    input_ids = torch.stack(tokenized_examples['input_ids'])\n","    attention_mask = torch.stack(tokenized_examples['attention_mask'])\n","\n","    input_ids = torch.transpose(input_ids, 0, 1).to(device)\n","    attention_mask = torch.transpose(attention_mask, 0, 1).to(device)\n","\n","    labels = tokenized_examples['label'].to(device)\n","\n","    out = model(input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels)\n","    \n","    curr_loss = out[\"loss\"]\n","\n","    curr_loss.backward()\n","    optimizer.step()\n","\n","\n","    if idx %5000==0:\n","      print(\"epoch \", i, \"batch \", idx, \", training loss: \", curr_loss.detach().to(\"cpu\").numpy())\n","  \n","\n","  #evaluation\n","  model.eval()\n","  total = 0\n","  matched = 0\n","  for idx, tokenized_examples in enumerate(tqdm(eval_loader)):\n","    input_ids = torch.stack(tokenized_examples['input_ids'])\n","    attention_mask = torch.stack(tokenized_examples['attention_mask'])\n","\n","    input_ids = torch.transpose(input_ids, 0, 1).to(device)\n","    attention_mask = torch.transpose(attention_mask, 0, 1).to(device)\n","    labels = tokenized_examples['label'].to(device)\n","\n","    pred = torch.argmax(model(input_ids=input_ids,\n","                attention_mask=attention_mask)[\"logits\"], dim=1)\n","\n","    total += len(input_ids)\n","    matched += (pred == labels).int().sum().cpu().numpy()\n","\n","  \n","  print(\"epoch \", i, \"test acc: \", matched/total)\n","  torch.save(model.state_dict(), model_dir+\"classifier\")\n","      \n"],"id":"H2FE0xhn58gG"},{"cell_type":"markdown","metadata":{"id":"k-WYcONdzYtu"},"source":[""],"id":"k-WYcONdzYtu"},{"cell_type":"markdown","metadata":{"id":"2FbgY8LVsS1X"},"source":[""],"id":"2FbgY8LVsS1X"}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"run.ipynb","provenance":[],"private_outputs":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}